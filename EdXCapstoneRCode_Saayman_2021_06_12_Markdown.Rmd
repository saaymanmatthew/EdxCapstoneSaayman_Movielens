---
output:
  pdf_document: default
  word_document: default
  html_document: default
---

```{r setup2, include=FALSE, ECHO = FALSE}
#Data Science Capstone project June 2021, Matthew Saayman . 
#The code for the PDF output file begins at line 671

##########################################################
# Create edx set, validation set (final hold-out test set)
##########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)



#MY CODE BEGINS HERE - Matthew Saayman, June 2021, EDX Data Science Capstone
####################################################################
#load appropriate libraries 
library(tidyverse)
library(caret)
library(data.table)
library(dslabs)
library(lubridate)
library(knitr)
#create a test and training set with 50% of the data assigned to each set
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.5, list = FALSE)
edx_train <- edx[-test_index,]
edx_test <- edx[test_index,]

#ensure the same users and movies in the test set appear in the training set
edx_test <- edx_test %>% 
  semi_join(edx_train, by = "movieId") %>%
  semi_join(edx_train, by = "userId")

#explore the data, beginning with movie and user counts

 
topedx <- head(edx)
tail(edx)
str(edx)
allusers <- unique(edx$userId)
alluserstrain <- unique(edx_train$userId)
length(allusers)
length(alluserstrain)
allmov <- unique(edx$movieId)
allmovtrain <- unique(edx_train$movieId)
length(allmov)
length(allmovtrain)
 
 
#compute three new columns for both test and train set
 
#First, compute a year_of_rating column. This is the year that the rating was made

edx_test$year_of_rating <- as.numeric(str_sub(round_date(as_datetime(edx_test$timestamp, origin="1970-01-01"), unit = "year"),1,4))
edx_train$year_of_rating <- as.numeric(str_sub(round_date(as_datetime(edx_train$timestamp, origin="1970-01-01"), unit = "year"),1,4))

#Second computer a year column. This is the year that the movie was released.

edx_test$year <- as.numeric(str_sub(edx_test$title, -5,-2))
edx_train$year <- as.numeric(str_sub(edx_train$title, -5,-2))

#Finally, compute a year difference column.
#This is the difference between the year of the user's rating and the year
# the movie was released

edx_test$yeardif <- edx_test$year_of_rating - edx_test$year
edx_train$yeardif <- edx_train$year_of_rating - edx_train$year

#computer a "weekday column" and a weekdayorder column for ordering the data

edx_test$weekday <- weekdays(round_date(as_datetime(edx_test$timestamp, origin="1970-01-01")))
edx_train$weekday <- weekdays(round_date(as_datetime(edx_train$timestamp, origin="1970-01-01")))
edx_test$weekdayorder <- ifelse(edx_test$weekday == "Sunday", 0, 
                                ifelse(edx_test$weekday == "Monday", 1,ifelse(edx_test$weekday == "Tuesday", 2,
                                                                              ifelse(edx_test$weekday == "Wednesday", 3,
                                                                                     ifelse(edx_test$weekday == "Thursday", 4,
                                                                                            ifelse(edx_test$weekday == "Friday", 5,
                                                                                                   ifelse(edx_test$weekday == "Saturday", 6,"")))))))
                                                                              
                                                                        

 
edx_train$weekdayorder <- ifelse(edx_train$weekday == "Sunday", 0, 
                                ifelse(edx_train$weekday == "Monday", 1,ifelse(edx_train$weekday == "Tuesday", 2,
                                                                              ifelse(edx_train$weekday == "Wednesday", 3,
                                                                                     ifelse(edx_train$weekday == "Thursday", 4,
                                                                                            ifelse(edx_train$weekday == "Friday", 5,
                                                                                                   ifelse(edx_train$weekday == "Saturday", 6,"")))))))


#computer an "hour column"
edx_test$hour <- str_sub(round_date(as_datetime(edx_test$timestamp, origin="1970-01-01"), unit = "hour"), -8,-7)
edx_train$hour <- str_sub(round_date(as_datetime(edx_train$timestamp, origin="1970-01-01"), unit = "hour"), -8,-7)


#Let's look if there are patterns related to these new variables

#Plotting the difference between the year of the movie release and 
#year of rating suggests there is some evidence of a time effect

edx_train %>% 
  group_by(year) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(year, rating)) +
  geom_point() +
  geom_smooth() +
  ggtitle("Figure 1- Average movie rating in each movie-release year")

edx_train %>% 
  group_by(year_of_rating) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(year_of_rating, rating)) +
  geom_point() +
  geom_smooth()+
  ggtitle("Figure 2- Average movie rating in each year of movie rating")


edx_train %>% 
  group_by(yeardif) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(yeardif, rating)) +
  geom_point() +
  geom_smooth() +
ggtitle("Figure 3- Average movie rating by year difference")

#The yeardif seems to be the most revealing chart. Not only is there a clear pattern,
#but the data also appears to more closely fit to the line

#What about examining by hour of the day? 

sample_n(edx_train, 10000)  %>% 
  group_by(hour) %>%
  summarize(count = n()) %>%
  ggplot(aes(hour,count)) +
  geom_col() +
  ggtitle("Figure 4 - Number of ratings by hour of the day, sample = 10,000")
  
#Above we can see that the busiest times for movie ratings are in the evening 
#(7 pm to 10 pm, or 19 to 22 on the chart)
#Least busy hour is 10 am

edx_train  %>%
  group_by(hour) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(hour,rating)) +
geom_point() +
  ggtitle("Figure 5 - Average rating by hour of the day")


#The above chart shows that the average rating per movie is affected by time of day

#Next, we can look at the effect of hte weekday. 

edx_train %>% 
  group_by(weekday) %>%
  summarize(count = n()) %>%
  ggplot(aes(x = (weekday), y= count)) +
  geom_col() +
  ggtitle("Figure 6 - Number of ratings by weekday")

#The above code shows that certain days of the week are busier for movie ratings. Monday and tuesday. And yet,
#the following code shows that Saturdays have the highest average ratings

edx_train %>% 
  group_by(weekday) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(weekday, rating)) +
  geom_point()  +
  ggtitle("Figure 7 - Average ratings for each week day")

#So far the code above has explored elements related to time, including year, day of the week, and hour of the day.
#What about examining the genre column? 

#First let's take a random sample of 1000 and plot average rating by genre

sample_n(edx_train, 1000) %>%
  group_by(genres) %>%
  summarize(rating = mean(rating), n = n()) %>%
  ggplot(aes(rating,genres)) +
  geom_point()




#The plot generated with the above code makes it difficult to observe a pattern for genres due to the
#numerous combinations of different genres for each movie. What about isolating each individual genre?

#Isolate genres

#What is the highest number of combinations of genres? 
#Answer will be presence of "|" (the separator) + 1

pattern <- (str_count(edx_train$genres, pattern = "[|]"))
max(pattern)
edx_train$genres[which.max(pattern)]
max(pattern) + 1 
edx_train$title[which.max(pattern)]

edx_train[which.max(pattern)]
edx_train[edx_train$title == "Host, The (Gwoemul) (2006)"]
# The Host (2006) has 8 genres

 #Now let's try to find the unique values for genre and plot their occurence in the dataset
mygen <- unique(edx_train$genres)
mygen <- data.frame(mygen = mygen)
mygen <- separate(mygen, mygen, into = c("Gen1","gen2","gen3","gen4","gen5","gen6","gen7","gen8"), sep = "[|]") 
mygen <- paste(c(mygen$Gen1,mygen$gen2,mygen$gen3,mygen$gen4,mygen$gen5,mygen$gen6,mygen$gen7), sep = ",")
allgenres <- unique(mygen)

#count occurences of different genres using sapply
#The function works as follows: 
#For each value in the list 'genreacounts', search for it in the column of 'genres'
#Then, provide a sum of all occurences for that value

genrecounts <- sapply(allgenres, function(x){
  a <- str_detect(edx_train$genres,x)
  sum(a)
})

as.vector(genrecounts)

#plot the count of different genres

genrecounts <- data.frame(genre = allgenres, count = as.vector(genrecounts))
genrecounts <- genrecounts %>% arrange(desc(count))
ggplot(genrecounts,aes(reorder(genre,count),count)) + geom_col() + ggtitle("Figure 8 Count of movie genres")

#The most common genres are Dramas, Comedies, Action, Thriller, Adventure, Romance,
#Science Fiction, Crime, Fantasy, Children
#what patterns can be observed looking at individual genres? 
#We can begin by looking at the top 5
 
#The code below filters according to the presence of one of the top 5 genres in the genre column,
#Then, the appearance of the top genre, Drama, results in that record recoded as Drama
#If it is not a drama, then the next value searched for is Comedy, then Action, and so on,
 
sample_n(edx_train,10000) %>% 
  group_by(userId, genres) %>% 
  filter(grepl('Drama',genres) |grepl('Comedy',genres) |grepl('Action',genres) 
         |grepl('Thriller',genres) |grepl('Adventure',genres)) %>%
  mutate(onegenre = if_else(str_detect(genres,'Drama') == TRUE,'Drama',
                            if_else(str_detect(genres,'Comedy') == TRUE,"Comedy", 
                                    if_else(str_detect(genres,'Action') == TRUE,"Action",
                                            if_else(str_detect(genres,'Thriller') == TRUE,'Thriller',
                                                    'Adventure'))))) %>%
  summarize(rating = mean(rating),onegenre) %>%
  ggplot(aes(onegenre, rating)) +
  geom_boxplot()+
  ggtitle("Figure 9 -  Ratings by Top 5 Genres")

#The obvious limitation of the above approach is that in the case where movies have multiple genres,
#they are being 'coerced' into one genre. A "Drama | Comedy" is coded here as a "Drama"
#and a "Comedy | Thrliler" coded as a Comedy.

#Nevertheless, a pattern emerges here with some movies having an overall higher median rating.
#The plot below shows the median values

edx_train %>% 
  group_by(genres) %>% 
  filter(grepl('Drama',genres)) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(genres, rating)) +
  geom_point() +
  geom_smooth()

#We can also pursue an alternative approach, looking for the word 'Action' for example in the genres column and then
#grouping accordingly. The plot below takes that approach

edx_train %>% 
  group_by(genres) %>% 
  filter(grepl('Action',genres)) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(genres, rating)) +
  geom_point() +
  geom_smooth()

edx_train %>% 
  group_by(genres) %>% 
  filter(grepl('Film-Noir',genres)) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(genres, rating)) +
  geom_point() +
  geom_smooth()

#What about separating out the Genres column to have a single genre in each new column?  
#As previously shown,
#a movie can have anywhere between 1 and 8 genres. 
#Hence the code below splits out the genres column
#into 8 new columns

temp_df <- separate(edx_train, genres, into = c("Gen1","gen2","gen3","gen4","gen5","gen6","gen7","gen8"), sep = "[|]", remove = FALSE)

 
#However, the problem with the above approach is that some columns will have many 'NA' values.
#Given the complexity of parsing out the genre column , we will treat each combination of genres as a genre in itself


#finally let's quickly look at the user and movie effects seen in the previous course machine learning, 
#section 6.2
#mean of ratings per user for users who had more than 50 reviews
edx_train %>% 
  group_by(userId) %>% 
  filter(n()>=50) %>%
  summarize(avg = mean(rating)) %>% 
  ggplot(aes(avg)) + 
  geom_histogram(bins = 30, color = "black")

#mean of ratings per movie for movies that had at least 50 reviews
edx_train %>% 
  group_by(movieId) %>% 
  filter(n()>=50) %>%
  summarize(avg = mean(rating)) %>% 
  ggplot(aes(avg)) + 
  geom_histogram(bins = 30, color = "black")

#MODELLING

#We can build a bias reflecting each of the effects before
#Hour effect
#Week effect
#Yeardif effect
#genre effect
#In addition, the movie and user effects from the Machine Learning course

#Below, model we begin by adding a single effect and then adding in additional effects 
#with each subsequent model


#model 1 - Weekday only
mu <- mean(edx_train$rating) 
weekdayavgst <- edx_train %>% 
  group_by(weekday) %>% 
  summarize(b_wd = mean(rating - mu))

predicted_ratings <- edx_test %>% 
  left_join(weekdayavgst, by='weekday') %>%
  mutate(pred = mu + b_wd) %>%
  pull(pred)
Model1RMSE <- RMSE(predicted_ratings, edx_test$rating)
#RMSE is  1.060155
 
#Model 2 - weekday and yeardif
yeardifavgst <- edx_train %>% 
  left_join(weekdayavgst, by='weekday') %>%
  group_by(yeardif) %>%
  summarize(b_yd = mean(rating - mu - b_wd))


predicted_ratings <- edx_test %>% 
  left_join(yeardifavgst, by='yeardif') %>%
  left_join(weekdayavgst, by='weekday') %>%
  mutate(pred = mu + b_yd + b_wd) %>%
  pull(pred)
Model2RMSE <- RMSE(predicted_ratings, edx_test$rating)
#RMSE is 1.051497


#Model 3 - weekday, yeardif and hour
houravvgst <- edx_train %>% 
  left_join(yeardifavgst, by='yeardif') %>%
  left_join(weekdayavgst, by='weekday') %>%
  group_by(hour) %>%
  summarize(b_h = mean(rating - mu - b_wd - b_yd))


predicted_ratings <- edx_test %>% 
  left_join(yeardifavgst, by='yeardif') %>%
  left_join(weekdayavgst, by='weekday') %>%
  left_join(houravvgst, by='hour') %>%
  mutate(pred = mu + b_yd + b_wd + b_h) %>%
  pull(pred)
Model3RMSE <- RMSE(predicted_ratings, edx_test$rating)
#RMSE is 1.051458

#Model 4 - weekday, yeardif, hour and genre 
genresavgt <- edx_train %>% 
  left_join(yeardifavgst, by='yeardif') %>%
  left_join(weekdayavgst, by = 'weekday') %>%
  left_join(houravvgst, by='hour') %>%
  group_by(genres) %>%
  summarize(b_g = mean(rating - mu - b_yd - b_wd - b_h))


predicted_ratings <- edx_test %>% 
  left_join(yeardifavgst, by='yeardif') %>%
  left_join(weekdayavgst, by='weekday') %>%
  left_join(houravvgst, by='hour') %>%
  left_join(genresavgt, by='genres') %>%
  mutate(pred = mu + b_yd + b_wd + b_h + b_g) %>%
  pull(pred)
Model4RMSE <- RMSE(predicted_ratings, edx_test$rating)
#RMSE is  1.010949



#Model 6 - weekday, yeardif, hour, genre , movie effect
movieavgst <- edx_train %>% 
  left_join(yeardifavgst, by='yeardif') %>%
  left_join(weekdayavgst, by = 'weekday') %>%
  left_join(houravvgst, by='hour') %>%
  left_join(genresavgt, by='genres') %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu - b_yd - b_wd - b_h - b_g))


predicted_ratings <- edx_test %>% 
  left_join(yeardifavgst, by='yeardif') %>%
  left_join(weekdayavgst, by='weekday') %>%
  left_join(houravvgst, by='hour') %>%
  left_join(genresavgt, by='genres') %>%
  left_join(movieavgst, by='movieId') %>%
  mutate(pred = mu + b_yd + b_wd + b_h + b_g + b_i) %>%
  pull(pred)
Model5RMSE <- RMSE(predicted_ratings, edx_test$rating)
#RMSE is 0.9443406

#Model 6 - weekday, yeardif, hour, genre , movie  effect and user effect
useravgst <- edx_train %>% 
  left_join(yeardifavgst, by='yeardif') %>%
  left_join(weekdayavgst, by = 'weekday') %>%
  left_join(houravvgst, by='hour') %>%
  left_join(genresavgt, by='genres') %>%
  left_join(movieavgst, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_yd - b_wd - b_h - b_g - b_i))


predicted_ratings <- edx_test %>% 
  left_join(yeardifavgst, by='yeardif') %>%
  left_join(weekdayavgst, by='weekday') %>%
  left_join(houravvgst, by='hour') %>%
  left_join(genresavgt, by='genres') %>%
  left_join(movieavgst, by='movieId') %>%
  left_join(useravgst, by='userId') %>%
  mutate(pred = mu + b_yd + b_wd + b_h + b_g + b_i + b_u) %>%
  pull(pred)
Model6RMSE <- RMSE(predicted_ratings, edx_test$rating)
#RMSE is  0.8700048

#Now, after training the model we can test on the edx set and on the validation set
# first we must ensure the Edx and validation sets have the same columns as in the training/test sets

#Compute a year_of_rating column. This is the year that the rating was made

edx$year_of_rating <- as.numeric(str_sub(round_date(as_datetime(edx$timestamp, origin="1970-01-01"), unit = "year"),1,4))
validation$year_of_rating <- as.numeric(str_sub(round_date(as_datetime(validation$timestamp, origin="1970-01-01"), unit = "year"),1,4))

#Computer a year column. This is the year that the movie was released.

edx$year <- as.numeric(str_sub(edx$title, -5,-2))
validation$year <- as.numeric(str_sub(validation$title, -5,-2))
#Compute a year difference column. This is the difference between the year of the user's rating and the year
# the movie was released

edx$yeardif <- edx$year_of_rating - edx$year
validation$yeardif <- validation$year_of_rating - validation$year 

#computer a "weekday column"

edx$weekday <- weekdays(round_date(as_datetime(edx$timestamp, origin="1970-01-01")))
validation$weekday <- weekdays(round_date(as_datetime(validation$timestamp, origin="1970-01-01")))
 
#computer an "hour column"
edx$hour <- str_sub(round_date(as_datetime(edx$timestamp, origin="1970-01-01"), unit = "hour"), -8,-7)
validation$hour <- str_sub(round_date(as_datetime(validation$timestamp, origin="1970-01-01"), unit = "hour"), -8,-7)

#Final_model tested on the edx  set
 
mu <- mean(edx$rating) 

weekdayavgs <- edx %>% 
  group_by(weekday) %>% 
  summarize(b_wd = mean(rating - mu))

yeardifavgs <- edx %>% 
  left_join(weekdayavgs, by='weekday') %>%
  group_by(yeardif) %>%
  summarize(b_yd = mean(rating - mu - b_wd))


houravvgs <- edx %>% 
  left_join(yeardifavgs, by='yeardif') %>%
  left_join(weekdayavgs, by='weekday') %>%
  group_by(hour) %>%
  summarize(b_h = mean(rating - mu - b_wd - b_yd))

genresavg <- edx %>% 
  left_join(yeardifavgs, by='yeardif') %>%
  left_join(weekdayavgs, by = 'weekday') %>%
  left_join(houravvgs, by='hour') %>%
  group_by(genres) %>%
  summarize(b_g = mean(rating - mu - b_wd - b_yd - b_h))


movieavgs <- edx %>% 
  left_join(yeardifavgs, by='yeardif') %>%
  left_join(weekdayavgs, by = 'weekday') %>%
  left_join(houravvgs, by='hour') %>%
  left_join(genresavg, by='genres') %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu - b_wd - b_yd - b_h - b_g))


useravgs <- edx %>% 
  left_join(yeardifavgs, by='yeardif') %>%
  left_join(weekdayavgs, by = 'weekday') %>%
  left_join(houravvgs, by='hour') %>%
  left_join(genresavg, by='genres') %>%
  left_join(movieavgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_wd - b_yd - b_h - b_g - b_i))


predicted_ratings <- edx %>% 
  left_join(movieavgs, by='movieId') %>%
  left_join(useravgs, by='userId') %>%
  left_join(genresavg, by='genres') %>%
  left_join(yeardifavgs, by='yeardif') %>%
  left_join(weekdayavgs, by = 'weekday') %>%
  left_join(houravvgs, by = 'hour') %>%
  mutate(pred = mu + b_wd + b_yd + b_h + b_g + b_i + b_u) %>%
  pull(pred)
Model6Edx <- RMSE(predicted_ratings, edx$rating)

#Test on the edx data set yields RMSE of  0.8567675

#Final test on the validation model
#confirm that all new columns have been computed
head(validation)

#Now test on the validation set

  mu <- mean(validation$rating) 
  
  weekdayavgs <- validation %>% 
    group_by(weekday) %>% 
    summarize(b_wd = mean(rating - mu))
  
  yeardifavgs <- validation %>% 
    left_join(weekdayavgs, by='weekday') %>%
    group_by(yeardif) %>%
    summarize(b_yd = mean(rating - mu - b_wd))
  
  
  houravvgs <- validation %>% 
    left_join(yeardifavgs, by='yeardif') %>%
    left_join(weekdayavgs, by='weekday') %>%
    group_by(hour) %>%
    summarize(b_h = mean(rating - mu - b_wd - b_yd))
  
  
  
  genresavg <- validation %>% 
    left_join(yeardifavgs, by='yeardif') %>%
    left_join(weekdayavgs, by = 'weekday') %>%
    left_join(houravvgs, by='hour') %>%
    group_by(genres) %>%
    summarize(b_g = mean(rating - mu - b_wd - b_yd - b_h))
  
  
  movieavgs <- validation %>% 
    left_join(yeardifavgs, by='yeardif') %>%
    left_join(weekdayavgs, by = 'weekday') %>%
    left_join(houravvgs, by='hour') %>%
    left_join(genresavg, by='genres') %>%
    group_by(movieId) %>%
    summarize(b_i = mean(rating - mu - b_wd - b_yd - b_h - b_g))
  
  
  useravgs <- validation %>% 
    left_join(yeardifavgs, by='yeardif') %>%
    left_join(weekdayavgs, by = 'weekday') %>%
    left_join(houravvgs, by='hour') %>%
    left_join(genresavg, by='genres') %>%
    left_join(movieavgs, by='movieId') %>%
    group_by(userId) %>%
    summarize(b_u = mean(rating - mu - b_wd - b_yd - b_h - b_g - b_i))
  
  
  predicted_ratings <- validation %>% 
    left_join(movieavgs, by='movieId') %>%
    left_join(useravgs, by='userId') %>%
    left_join(genresavg, by='genres') %>%
    left_join(yeardifavgs, by='yeardif') %>%
    left_join(weekdayavgs, by = 'weekday') %>%
    left_join(houravvgs, by = 'hour') %>%
    mutate(pred = mu + b_wd + b_yd + b_h + b_g + b_i + b_u) %>%
    pull(pred)
  Model6Valid <- RMSE(predicted_ratings, validation$rating)
 
#Final Model test on validation yields RMSE of .82
 
   






```
 
---
output:
  pdf_document: default
  html_document: default
---
---
title: "Edx - Data Science Capstone - Paradox of Choice and Machine Learning"
author: "Matthew Saayman"
date: "11/06/2021"
output:
  pdf_document: default
  word_document: default
---

```{r setup3, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache =TRUE)
```
**Introduction** 

The Paradox of Choice, as described by psychologist Barry Schwartz, refers to the paralysis and anxiety induced by an over-abundance of choice. We can observe this phenomenon in the physical world, in the case of a store being stocked with countless brands, and in the digital world, in the realm of online dating or online movie streaming. The experience of some Netflix users struggling to decide the right movie to watch is hardly uncommon. This effect negatively impacts decision making with some users even eschewing a process or product altogether (https://tophotel.news/experts-voice-how-to-upsell-101-too-much-choice-can-lead-to-no-choice/). In that light, correctly predicting what a consumer will enjoy is important for human well-being.

Creating a movie recommendation system can be seen as one way to address this paradox and represents a useful study in machine learning. This paper uses a sample of the movielens dataset to build a linear regression model for movie recommendations and includes the following biases: weekday, hour of the day, year difference, genre, user, and movie.

The first section describes the dataset in greater detail and the technical challenges with working with the data. In addition, an exploration of the data is provided along with visualizations to demonstrate the existence of certain effects on ratings. The second section describes the model in greater detail and provides a justification for why the given method was used. The final section describes the results of the model on the validation dataset and concludes with a discussion on future uses for the model and its limitations.

**Section 1. Methods and Analysis – Data Exploration**

The movielens dataset is freely available dataset containing millions observations, with each row counting as a rating by one user of one movie. As part of the edX capstone project, a sample of one version of the Movielens dataset, containing 10 million rows, is created as “edx”, containing 9,000,055 observations including 69,878 unique users of  10,677 unique movies.

A training and dataset were then partitioned from the edx set, with 50% going to each set.
The structure of the edX dataset appears in Figure 1 (below), with movieId representing the movie that was rated, userId representing the individual who rated the movie, rating (from 0.5 to 5 stars), the timestamp representing the date and time the rating was made, title of the movie, and genres of the movie.

In examining the first few rows of the edx dataset it becomes immediately clear that we will face technical challenges in building the model. Timestamp is stored as the number of seconds since 1970/01/01. Genres has multiple genres , with some movies having a single genre (ie// “Drama”) and others having multiple (“Drama | comedy”).

Figure 1

```{r edx_train0, echo = FALSE, eval = TRUE}
topedx
```

**1.2 Exploration of the time effect**

Before building the model we can ask ourselves questions about the interaction of the columns in the dataset based on general knowledge of human behaviour. We can start by exploring the training data set to see evidence of “time effect”, specifically looking at and manipulating the timestamp column.
How are average ratings affected by:

-The time of the day? 

-The day of the week?

-The current year?

Netflix’s algorithm takes into account the time of day a user watches a movie (https://help.netflix.com/en/node/100639), and it seems plausible that the time of day (or week) would influence a user’s mood and therefore the rating they offer a movie. In addition, as noted in the edx Machine Learning Course (6.2) assessment, the year of the movie release may play a role too. A newer movie will have a lower rating on average because fewer users have had the opportunity to rate it, compared to an older movie for which there has been additional time for users to rate it.

Accordingly, we can create new variables using the timestamp column and the title column.

-Year: extracted from the title column, this is the year the movie was released.

-Year of rating: this is the year the user made their rating.

-YearDif: Year of the rating minus the year of the movie release.

-Weekday: The day of the week the rating was made (values “Sunday”, “Monday”, etc.).

-Hour: The time of day the rating was made (values are 0 to 23 with 0 representing midnight).

The following plot (Figure 2) shows how the average rating varies according to movie release year


```{r edx_train1, echo=FALSE, message=FALSE, warning = FALSE}
edx_train %>% 
  group_by(year) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(year, rating)) +
  geom_point() +
  geom_smooth() +
  ggtitle("Figure 2 - Average movie rating by  each movie-release year")
```

Here, each data point represents a given movie-release year and the average rating of all movie-ratings in that year, the line represents the general trend when the data is grouped by movie-release year, and the shaded area around the line represents the confidence intervals for each data point. Although the general pattern presented here suggests newer movies do indeed get lower ratings, on average, it is also clear there is much variation with so many data points outside the bounds of the confidence intervals.

```{r edx_train2, echo=FALSE, message=FALSE, warning = FALSE}
edx_train %>% 
  group_by(year_of_rating) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(year_of_rating, rating)) +
  geom_point() +
  geom_smooth()+
  ggtitle("Figure 3 - Average movie rating in each year of movie rating")
```

When plotting the year of the rating, there is an altogether different pattern (Figure 3). As the dataset is for ratings from 1995 onwards, we can see that after the year 1995 the average sharply decreases each subsequent year, likely a reflection of an increased number of users in the dataset and ratings in the system. By the year 2000, the line is relatively stable which makes sense as we should hardly expect that users are more or less critical (on average) year over year.


```{r edx_train3, echo=FALSE, message=FALSE, warning = FALSE}
edx_train %>% 
  group_by(yeardif) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(yeardif, rating)) +
  geom_point() +
  geom_smooth() +
ggtitle("Figure 4 - Average movie rating by year difference")
```

When plotting the “Year Diff” column (the difference between the year of a rating and the year the movie was released), a clear pattern emerges (Figure 4). When the year difference is < 1 years between the rating year and the movie release year, the average rating is lower. But the average rating increases for each additional year, before decreasing at around 75 years. What this plot essentially demonstrates is that, just as in Figure 2, average ratings for newer movies will generally be lower than older movies, holding all else equal.There is of course variation within this plot but the data appears to more closely gather around the line than in Figure 2. 

We should therefore consider adding this YearDiff variable to the model.

What about examining the time of day? It is not clear from the dataset if all users were in the same timezone or even the same geographic area when they submitted the rating. Nevertheless, a clear pattern emerges when we look at the average rating according to hour of the day.

```{r edx_train4, echo=FALSE, message=FALSE, warning = FALSE}
sample_n(edx_train, 10000)  %>% 
  group_by(hour) %>%
  summarize(count = n()) %>%
  ggplot(aes(hour,count)) +
  geom_col() +
  ggtitle("Figure 5 - Number of ratings by hour of the day")
```

Figure 5 shows that the busiest hours for submitting ratings were between 7 and 10 (19 and 22 on the chart, respectively), with the least busy hours between 8 and 11 am.  Figure 6 shows that the average rating varies somewhat according to the hour of the day.

 

```{r edx_train5, echo=FALSE, message=FALSE, warning = FALSE}
edx_train  %>%
  group_by(hour) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(hour,rating)) +
geom_point() +
  ggtitle("Figure 6 - Average rating by hour of the day")
```

This effect may not appear as strong as the “year difference” effect but we could still consider adding it to the model as well.

Finally, what about the day of the week? We should expect more users to submit more ratings on weekends, potentially affecting the rating.

Figure 7 shows that Monday and Tuesday are the busiest days for movie ratings. Figure 8 shows that the average rating differs by day as well, with the average rating highest for Saturdays and lowest for Thursdays. 

```{r edx_train6, echo=FALSE, message=FALSE, warning = FALSE}
edx_train %>% 
  group_by(weekday) %>%
  summarize(count = n()) %>%
  ggplot(aes(weekday, count)) +
  geom_col() +
  ggtitle("Figure 7 - Number of ratings by weekday")

```
```{r edx_train7, echo=FALSE,message=FALSE, warning = FALSE}
edx_train %>% 
  group_by(weekday) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(weekday, rating)) +
  geom_point()  +
  ggtitle("Figure 8 - Average ratings for each week day")
```

**1.3  - Exploration of the Genre effect**

We have so far examined the effect timing may have on the rating, specifically year, hour and weekday. 
What about genre?

Intuitively we know some genres are more popular than others, so how does the genre impact the rating of a movie?

The genre column in the dataset presents us with a technical challenge as there are multiple genres for some movies. How can we account for the effects of different genres on the rating? Or should each combination of genre (Science Fiction | Drama) be considered as a genre in of itself? One movie, The Host (2008) has 8 associated genres, highlighting the challenge of reorganizing this data:

Action|Adventure|Comedy|Drama|Fantasy|Horror|Sci-Fi|Thriller

We can begin by breaking up the genres column to see what are the unique values and how often they occur. The following plot shows the 20 unique values in the genres column and their occurrences throughout the training set.

```{r edx_train8, echo=FALSE,message=FALSE, warning = FALSE}
genrecounts %>% filter(count > 500000) %>% ggplot(aes(reorder(genre,count),count)) + geom_col() + ggtitle("Figure 9 - Top 8 Genres") +  labs(x = "Genre")
```
The most common 8 genres are Dramas, Comedies, Action, Thriller, Adventure, Romance,
Science Fiction, and Crime. What patterns can be observed looking at individual genres? For expediency, we look at the top 5 only (Figure 10).

```{r edx_train9, echo=FALSE,message=FALSE, warning = FALSE}
sample_n(edx_train,10000) %>% 
  group_by(userId, genres) %>% 
  filter(grepl('Drama',genres) |grepl('Comedy',genres) |grepl('Action',genres) 
         |grepl('Thriller',genres) |grepl('Adventure',genres)) %>%
  mutate(onegenre = if_else(str_detect(genres,'Drama') == TRUE,'Drama',
                            if_else(str_detect(genres,'Comedy') == TRUE,"Comedy", 
                                    if_else(str_detect(genres,'Action') == TRUE,"Action",
                                            if_else(str_detect(genres,'Thriller') == TRUE,'Thriller',
                                                    'Adventure'))))) %>%
  summarize(rating = mean(rating),onegenre) %>%
  ggplot(aes(onegenre, rating)) +
  geom_boxplot()  + ggtitle("Figure 10 - Median Ratings by Top 5 Genres") +labs(x = "Genre")
```

The limitation of the above approach is that in the case where movies have multiple genres,
they are being 'coerced' into one genre. A "Drama | Comedy" is coded here as a "Drama"
and a "Comedy | Thriller" coded as a Comedy, and so on. Yet, a pattern emerges here with Dramas having a median rating of 4 and action adventure, and comedy having medians of 3.5.

An alternative approach is to split out the genres column into eight new columns (eight being the highest number of genres for a movie), with a new column representing the first genre identified for each movie. Here a clear limitation is that each column “Gen1”, “Gen2”, etc. will have different values within it. 

```{r edx_train10, echo=FALSE,message=FALSE, warning = FALSE}
head(temp_df)
```

In light of the limitations  mentioned in wrangling the genre column, our approach will be to use the genre column ‘as is’ and treat each genre combination as a genre in of itself.

**1.3  - Exploration of  User and Movie effects**

Finally, we can consider adding in the previously examined user and movie effects from the Machine Learning course, section 6.2

We know from that module that there is variability in terms of the number of ratings that users submit and in their ratings.

Some users may be overcritical while others may be the complete opposite and give every movie a 5-star rating. 

Similarly, some movies will receive higher ratings and others, lower ratings.

```{r edx_train11, echo=FALSE,message=FALSE, warning = FALSE}
edx_train %>% 
  group_by(userId) %>% 
  filter(n()>=50) %>%
  summarize(avg = mean(rating)) %>% 
  ggplot(aes(avg)) + 
  geom_histogram(bins = 30, color = "black") + ggtitle("Figure 11 - Average rating per users who had 50+ reviews")
```

```{r edx_train12, echo=FALSE}
edx_train %>% 
  group_by(movieId) %>% 
  filter(n()>=50) %>%
  summarize(avg = mean(rating)) %>% 
  ggplot(aes(avg)) + 
  geom_histogram(bins = 30, color = "black") + ggtitle("Figure 12 - Average rating per movie for movies with 50+ reviews")
```

Our model must take all of these effects into account. 


**Section 2. Methods**

So far we have described the Edx dataset in detail and demonstrated how recoding certain variables or creating new ones can reveal patterns about movie ratings. How can this information be added to a machine learning model?

Ultimately, our aim is to predict how a user will rate a movie, between 0.5 stars and 5 stars, based on their past ratings. The model we build will use our training/test sets and eventually be tested on the validation ‘final hold-out test-set’ to predict the real ratings as if they are unknown.When tested on the validation set, the Residual Mean Square Error (RMSE) should be less than 0.86490. 

Since the aim is prediction and the output is a number, it seems appropriate that we classify this as a regression problem (https://towardsdatascience.com/do-you-know-how-to-choose-the-right-machine-learning-algorithm-among-7-different-types-295d0b0c7f60). While more advance modelling techniques could be used, such as those submitted as part of the competition for the ‘Netflix Prize’ (http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/ ), when this author attempted to process a random forest involving 1% of the edx set they were unable to due to the technical limitations of their computer. A regression model may not be the most advanced method but it may still allow us to accomplish our goal of obtaining an RMSE of <.86490.

Similar to the algorithm outlined in section 6.2 of the machine learning course, we proceed here to test the effect of one bias and then add in additional biases one at a time, observing how the RMSE changes in each model.Six models are developed, each one being built with the training set and tested against the test set. The sixth model is then applied to the edx dataset and then to the validation dataset.

We begin by identifying the mean of ratings in the training set and then add in each bias to improve the predictive power of the model.

mu = the mean of all ratings.  

B_wd = the weekday bias.  

This is the average rating for each weekday.  

We obtain it as follows:  

-Group the data by weekday.  
-Mean(Each individual rating – mu).  

B_yd = the year difference bias.  
This is the average rating for each year difference (between the year of rating and the year of release).  
We obtain it as follows:  

-Group by yeardiff.  
-Mean(each individual rating – mu – b_wd).  

B_h = the hour bias.  
This is the average rating for each hour.  
We obtain it as follows:  

-Group by hour.  
-Mean(each individual rating - movie rating – mu – b_wd – b_yd).  

b_g = the genre bias.  
This is the average rating for each genre combination.  
We obtain it as follows:  

-Group by genres.  
-Mean(each individual rating – mu – b_wd – b_yd – b_h).

b_i = the movie bias.  
This is the average rating for every movie.  
We obtain it as follows:  

-Group by movie_id.  
-Mean(each individual rating – mu – b_wd – b_yd – b_h – b_g).

b_u = the user bias.   
The average rating made by every user.  
We obtain it as follows:  

-Group by userId.  
-Mean(each individual rating – mu – b_wd – b_yd – b_h – b_g - b_i).



**Section 2 - Models**

Model 1: mu + b_wd.  
RMSE is  1.060155.


Model 2: Model 1 + b_yd.  
RMSE is 1.051497.


Model 3: Model 2 + b_h.  
RMSE is 1.051458.


Model 4: Model 3 + b_g.  
RMSE is  1.010949.


Model5: Model 4 + b_i.  
RMSE is 0.9443406.


Model 6: mu + b_wd + b_yd + b_h + b_g + b_i + bu.  
RMSE is 0.8700048.



Section 3 – Results

Our final model used (Model 6) followed an identical methodology to the model developed in section 6.2 of the machine learning course. The model in that section used the user effect and the movie effect to produce an RMSE of 0.905 (without regularization)

When Model 6 was tested on the edx set, the RMSE was **0.8567675**.  
When Model 6 was tested on the validation ‘final hold-out’ set, the RMSE was **0.8252246**.  
We have successfully met our goal of obtaining an RMSE of < .86490.


**Section 4. Discussion**  

Methodologically, we have followed a straightforward approach while still offering a model that has predictive power. The model did not come without its limitations. For example, it would be interesting to gain access a new version of the dataset (or a different one altogether) that includes columns such as gender and age. There are countless other biases that could have been added to the model. Another limitation is our use of the genre column. More advanced wrangling techniques might be able to properly isolate the individual genre. Finally, advanced equipment could permit experimenting with other modelling techniques (random forests). 

**Conclusion** 

This paper has presented a linear regression model for predicting how a given user will rate a given movie by making use of a sample of the Movielens dataset. During an initial data exploration, we computed new columns and discovered new patterns related to:

-The year in which the movie was made  

-The year the rating was made, and  

-The difference between them  

-The week day on which the rating was made  

-The hour of the day in which the rating was made  

We saw how such time-related factors influence the average ratings for a movie.  


We also attempted to examine the effects of genres on average ratings.

Several biases were created and added one at a time to the training model which were applied to the test set. A final model involving six biases was tested on the validation set to produce an RMSE of 0.8252246.